#!/usr/bin/env python3
"""
æœ¬åœ°ç·©å­˜æ¨¡å¡Š
ç‚ºå¢å¼·ç‰ˆRAGç³»çµ±æä¾›æœ¬åœ°ç·©å­˜åŠŸèƒ½
"""

import os
import json
import hashlib
import time
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime


class CacheManager:
    """ç·©å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = None, ttl_hours: int = 24):
        """
        åˆå§‹åŒ–ç·©å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç·©å­˜ç›®éŒ„è·¯å¾‘ï¼Œé»˜èªç‚º ~/.cache/rag-system
            ttl_hours: ç·©å­˜ç”Ÿå­˜æ™‚é–“ï¼ˆå°æ™‚ï¼‰ï¼Œé»˜èª24å°æ™‚
        """
        if cache_dir is None:
            cache_dir = os.path.expanduser("~/.cache/rag-system")
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl_seconds = ttl_hours * 3600
        
        # ç·©å­˜çµ±è¨ˆ
        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "evictions": 0,
            "errors": 0
        }
        
        # ç·©å­˜ç´¢å¼•æ–‡ä»¶
        self.index_file = self.cache_dir / "cache_index.json"
        self._load_index()
    
    def _load_index(self):
        """åŠ è¼‰ç·©å­˜ç´¢å¼•"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    self.index = json.load(f)
            except:
                self.index = {}
        else:
            self.index = {}
    
    def _save_index(self):
        """ä¿å­˜ç·©å­˜ç´¢å¼•"""
        try:
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(self.index, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ä¿å­˜ç·©å­˜ç´¢å¼•å¤±æ•—: {e}")
    
    def _generate_cache_key(self, project_path: str, cache_type: str, 
                           content_hash: str = None) -> str:
        """
        ç”Ÿæˆç·©å­˜éµ
        
        Args:
            project_path: é …ç›®è·¯å¾‘
            cache_type: ç·©å­˜é¡å‹ï¼ˆanalysis, processing, learning, decision, packagingï¼‰
            content_hash: å…§å®¹å“ˆå¸Œï¼ˆå¯é¸ï¼‰
        
        Returns:
            ç·©å­˜éµå­—ç¬¦ä¸²
        """
        # æ¨™æº–åŒ–é …ç›®è·¯å¾‘
        project_path = os.path.abspath(project_path)
        
        # å¦‚æœæ²’æœ‰æä¾›å…§å®¹å“ˆå¸Œï¼Œç”Ÿæˆä¸€å€‹åŸºæ–¼è·¯å¾‘å’Œæ™‚é–“çš„å“ˆå¸Œ
        if content_hash is None:
            content_hash = hashlib.md5(
                f"{project_path}:{time.time():.0f}".encode()
            ).hexdigest()[:8]
        
        # ç”Ÿæˆç·©å­˜éµ
        key_parts = [
            cache_type,
            hashlib.md5(project_path.encode()).hexdigest()[:12],
            content_hash[:8]
        ]
        
        return "_".join(key_parts)
    
    def _get_project_hash(self, project_path: str) -> str:
        """
        è¨ˆç®—é …ç›®å“ˆå¸Œå€¼
        
        Args:
            project_path: é …ç›®è·¯å¾‘
        
        Returns:
            é …ç›®å“ˆå¸Œå­—ç¬¦ä¸²
        """
        project_path = Path(project_path)
        
        # æ”¶é›†é—œéµæ–‡ä»¶ä¿¡æ¯
        file_info = []
        
        # æª¢æŸ¥é—œéµæ–‡ä»¶
        critical_files = [
            "package.json",
            "README.md",
            "tsconfig.json",
            "app.json",
            "index.js",
            "App.js",
            "App.tsx"
        ]
        
        for file_name in critical_files:
            file_path = project_path / file_name
            if file_path.exists():
                try:
                    # ç²å–æ–‡ä»¶ä¿®æ”¹æ™‚é–“å’Œå¤§å°
                    stat = file_path.stat()
                    file_info.append(f"{file_name}:{stat.st_mtime}:{stat.st_size}")
                except:
                    pass
        
        # ç²å–ç›®éŒ„çµæ§‹ä¿¡æ¯
        try:
            dirs = []
            files = []
            for root, dirnames, filenames in os.walk(project_path):
                # åªè€ƒæ…®å‰å…©å±¤ç›®éŒ„
                depth = root[len(str(project_path)):].count(os.sep)
                if depth <= 2:
                    dirs.extend(dirnames)
                    files.extend(filenames[:20])  # åªå–å‰20å€‹æ–‡ä»¶
            
            dirs.sort()
            files.sort()
            file_info.append(f"dirs:{','.join(dirs[:10])}")
            file_info.append(f"files:{','.join(files[:20])}")
        except:
            pass
        
        # ç”Ÿæˆå“ˆå¸Œ
        if file_info:
            content = ":".join(file_info)
            return hashlib.md5(content.encode()).hexdigest()[:16]
        else:
            return hashlib.md5(str(project_path).encode()).hexdigest()[:16]
    
    def _get_cache_file_path(self, cache_key: str) -> Path:
        """ç²å–ç·©å­˜æ–‡ä»¶è·¯å¾‘"""
        # ä½¿ç”¨å‰å…©å€‹å­—ç¬¦ä½œç‚ºå­ç›®éŒ„ï¼Œé¿å…å–®å€‹ç›®éŒ„æ–‡ä»¶éå¤š
        subdir = cache_key[:2]
        subdir_path = self.cache_dir / subdir
        subdir_path.mkdir(exist_ok=True)
        
        return subdir_path / f"{cache_key}.json"
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆæœªéæœŸï¼‰"""
        if cache_key not in self.index:
            return False
        
        cache_info = self.index[cache_key]
        created_time = cache_info.get("created_at", 0)
        
        # æª¢æŸ¥æ˜¯å¦éæœŸ
        if time.time() - created_time > self.ttl_seconds:
            return False
        
        # æª¢æŸ¥ç·©å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        cache_file = self._get_cache_file_path(cache_key)
        return cache_file.exists()
    
    def _clean_expired_cache(self):
        """æ¸…ç†éæœŸç·©å­˜"""
        current_time = time.time()
        expired_keys = []
        
        for cache_key, cache_info in self.index.items():
            created_time = cache_info.get("created_at", 0)
            if current_time - created_time > self.ttl_seconds:
                expired_keys.append(cache_key)
        
        for cache_key in expired_keys:
            self._delete_cache(cache_key)
            self.stats["evictions"] += 1
        
        if expired_keys:
            print(f"æ¸…ç†äº† {len(expired_keys)} å€‹éæœŸç·©å­˜")
    
    def _delete_cache(self, cache_key: str):
        """åˆªé™¤ç·©å­˜"""
        try:
            # åˆªé™¤ç·©å­˜æ–‡ä»¶
            cache_file = self._get_cache_file_path(cache_key)
            if cache_file.exists():
                cache_file.unlink()
            
            # å¾ç´¢å¼•ä¸­ç§»é™¤
            if cache_key in self.index:
                del self.index[cache_key]
            
            return True
        except Exception as e:
            print(f"åˆªé™¤ç·©å­˜å¤±æ•— {cache_key}: {e}")
            self.stats["errors"] += 1
            return False
    
    def get(self, project_path: str, cache_type: str) -> Optional[Dict[str, Any]]:
        """
        ç²å–ç·©å­˜æ•¸æ“š
        
        Args:
            project_path: é …ç›®è·¯å¾‘
            cache_type: ç·©å­˜é¡å‹
        
        Returns:
            ç·©å­˜æ•¸æ“šæˆ–None
        """
        # è¨ˆç®—é …ç›®å“ˆå¸Œ
        project_hash = self._get_project_hash(project_path)
        
        # ç”Ÿæˆç·©å­˜éµ
        cache_key = self._generate_cache_key(project_path, cache_type, project_hash)
        
        # æª¢æŸ¥ç·©å­˜æœ‰æ•ˆæ€§
        if not self._is_cache_valid(cache_key):
            self.stats["misses"] += 1
            return None
        
        try:
            # è®€å–ç·©å­˜æ–‡ä»¶
            cache_file = self._get_cache_file_path(cache_key)
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ›´æ–°è¨ªå•æ™‚é–“
            if cache_key in self.index:
                self.index[cache_key]["last_accessed"] = time.time()
                self._save_index()
            
            self.stats["hits"] += 1
            return cache_data.get("data")
        
        except Exception as e:
            print(f"è®€å–ç·©å­˜å¤±æ•— {cache_key}: {e}")
            self.stats["errors"] += 1
            self.stats["misses"] += 1
            return None
    
    def set(self, project_path: str, cache_type: str, data: Dict[str, Any]) -> bool:
        """
        è¨­ç½®ç·©å­˜æ•¸æ“š
        
        Args:
            project_path: é …ç›®è·¯å¾‘
            cache_type: ç·©å­˜é¡å‹
            data: è¦ç·©å­˜çš„æ•¸æ“š
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # è¨ˆç®—é …ç›®å“ˆå¸Œ
        project_hash = self._get_project_hash(project_path)
        
        # ç”Ÿæˆç·©å­˜éµ
        cache_key = self._generate_cache_key(project_path, cache_type, project_hash)
        
        try:
            # æº–å‚™ç·©å­˜æ•¸æ“š
            cache_data = {
                "key": cache_key,
                "project_path": os.path.abspath(project_path),
                "cache_type": cache_type,
                "project_hash": project_hash,
                "created_at": time.time(),
                "data": data
            }
            
            # å¯«å…¥ç·©å­˜æ–‡ä»¶
            cache_file = self._get_cache_file_path(cache_key)
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            
            # æ›´æ–°ç´¢å¼•
            self.index[cache_key] = {
                "project_path": os.path.abspath(project_path),
                "cache_type": cache_type,
                "project_hash": project_hash,
                "created_at": time.time(),
                "last_accessed": time.time(),
                "size": cache_file.stat().st_size if cache_file.exists() else 0
            }
            
            self._save_index()
            self.stats["sets"] += 1
            
            return True
        
        except Exception as e:
            print(f"è¨­ç½®ç·©å­˜å¤±æ•— {cache_key}: {e}")
            self.stats["errors"] += 1
            return False
    
    def delete(self, project_path: str = None, cache_type: str = None) -> int:
        """
        åˆªé™¤ç·©å­˜
        
        Args:
            project_path: é …ç›®è·¯å¾‘ï¼ˆå¯é¸ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰é …ç›®ï¼‰
            cache_type: ç·©å­˜é¡å‹ï¼ˆå¯é¸ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰é¡å‹ï¼‰
        
        Returns:
            åˆªé™¤çš„ç·©å­˜æ•¸é‡
        """
        deleted_count = 0
        
        # æ”¶é›†è¦åˆªé™¤çš„ç·©å­˜éµ
        keys_to_delete = []
        
        for cache_key, cache_info in self.index.items():
            match_project = (project_path is None or 
                           os.path.abspath(project_path) == cache_info.get("project_path"))
            match_type = (cache_type is None or 
                         cache_type == cache_info.get("cache_type"))
            
            if match_project and match_type:
                keys_to_delete.append(cache_key)
        
        # åˆªé™¤ç·©å­˜
        for cache_key in keys_to_delete:
            if self._delete_cache(cache_key):
                deleted_count += 1
        
        return deleted_count
    
    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç·©å­˜"""
        deleted_count = 0
        
        # åˆªé™¤æ‰€æœ‰ç·©å­˜æ–‡ä»¶
        for cache_file in self.cache_dir.rglob("*.json"):
            try:
                cache_file.unlink()
                deleted_count += 1
            except:
                pass
        
        # æ¸…ç©ºç´¢å¼•
        self.index = {}
        self._save_index()
        
        # é‡ç½®çµ±è¨ˆ
        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "evictions": 0,
            "errors": 0
        }
        
        print(f"æ¸…ç©ºäº† {deleted_count} å€‹ç·©å­˜æ–‡ä»¶")
        return deleted_count
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–ç·©å­˜çµ±è¨ˆä¿¡æ¯"""
        total_cached = len(self.index)
        
        # è¨ˆç®—ç·©å­˜å¤§å°
        total_size = 0
        for cache_info in self.index.values():
            total_size += cache_info.get("size", 0)
        
        # è¨ˆç®—å‘½ä¸­ç‡
        total_access = self.stats["hits"] + self.stats["misses"]
        hit_rate = (self.stats["hits"] / total_access * 100) if total_access > 0 else 0
        
        return {
            "total_cached": total_cached,
            "total_size_bytes": total_size,
            "total_size_mb": total_size / (1024 * 1024),
            "hits": self.stats["hits"],
            "misses": self.stats["misses"],
            "sets": self.stats["sets"],
            "evictions": self.stats["evictions"],
            "errors": self.stats["errors"],
            "hit_rate_percent": round(hit_rate, 2),
            "cache_dir": str(self.cache_dir)
        }
    
    def list_cached_projects(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰ç·©å­˜çš„é …ç›®"""
        projects = {}
        
        for cache_key, cache_info in self.index.items():
            project_path = cache_info.get("project_path")
            if project_path not in projects:
                projects[project_path] = {
                    "path": project_path,
                    "cache_types": [],
                    "total_size": 0,
                    "last_accessed": 0
                }
            
            projects[project_path]["cache_types"].append(cache_info.get("cache_type"))
            projects[project_path]["total_size"] += cache_info.get("size", 0)
            
            last_accessed = cache_info.get("last_accessed", 0)
            if last_accessed > projects[project_path]["last_accessed"]:
                projects[project_path]["last_accessed"] = last_accessed
        
        # è½‰æ›ç‚ºåˆ—è¡¨ä¸¦æ’åº
        project_list = list(projects.values())
        project_list.sort(key=lambda x: x["last_accessed"], reverse=True)
        
        # æ ¼å¼åŒ–æ™‚é–“
        for project in project_list:
            if project["last_accessed"] > 0:
                dt = datetime.fromtimestamp(project["last_accessed"])
                project["last_accessed_str"] = dt.strftime("%Y-%m-%d %H:%M:%S")
            else:
                project["last_accessed_str"] = "å¾æœªè¨ªå•"
            
            project["total_size_mb"] = project["total_size"] / (1024 * 1024)
        
        return project_list


class CachedRAGAnalyzer:
    """å¸¶ç·©å­˜çš„RAGåˆ†æå™¨"""
    
    def __init__(self, project_path: str, cache_manager: CacheManager = None):
        self.project_path = project_path
        
        if cache_manager is None:
            cache_manager = CacheManager()
        self.cache_manager = cache_manager
        
        # å°å…¥åŸå§‹åˆ†æå™¨
        from rag_analyzer import ProjectAnalyzer
        self.analyzer = ProjectAnalyzer(project_path)
    
    def generate_analysis_report(self, use_cache: bool = True) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†æå ±å‘Šï¼ˆå¸¶ç·©å­˜ï¼‰"""
        # å˜—è©¦å¾ç·©å­˜ç²å–
        if use_cache:
            cached_data = self.cache_manager.get(self.project_path, "analysis")
            if cached_data is not None:
                print("âœ… å¾ç·©å­˜åŠ è¼‰åˆ†æå ±å‘Š")
                return cached_data
        
        # åŸ·è¡Œå¯¦éš›åˆ†æ
        print("ğŸ” åŸ·è¡ŒRAGåˆ†æ...")
        report = self.analyzer.generate_analysis_report()
        
        # ä¿å­˜åˆ°ç·©å­˜
        if use_cache:
            self.cache_manager.set(self.project_path, "analysis", report)
            print("ğŸ’¾ åˆ†æå ±å‘Šå·²ç·©å­˜")
        
        return report
    
    def save_report(self, report: Dict[str, Any], output_path: str = None) -> str:
        """ä¿å­˜å ±å‘Šï¼ˆåŒæ™‚æ›´æ–°ç·©å­˜ï¼‰"""
        # ä¿å­˜åˆ°æ–‡ä»¶
        result_path = self.analyzer.save_report(report, output_path)
        
        # æ›´æ–°ç·©å­˜
        self.cache_manager.set(self.project_path, "analysis", report)
        
        return result_path


def main():
    """æ¸¬è©¦ç·©å­˜ç®¡ç†å™¨"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python cache_manager.py <é …ç›®è·¯å¾‘>")
        print("ç¤ºä¾‹: python cache_manager.py /path/to/your/project")
        sys.exit(1)
    
    project_path = sys.argv[1]
    
    if not os.path.exists(project_path):
        print(f"éŒ¯èª¤: é …ç›®è·¯å¾‘ä¸å­˜åœ¨: {project_path}")
        sys.exit(1)
    
    print("ğŸ§ª æ¸¬è©¦ç·©å­˜ç®¡ç†å™¨")
    print("=" * 50)
    
    # å‰µå»ºç·©å­˜ç®¡ç†å™¨
    cache_manager = CacheManager()
    
    # æ¸¬è©¦ç·©å­˜çµ±è¨ˆ
    stats = cache_manager.get_stats()
    print(f"ç·©å­˜ç›®éŒ„: {stats['cache_dir']}")
    print(f"ç·©å­˜é …ç›®æ•¸: {stats['total_cached']}")
    print(f"ç·©å­˜å¤§å°: {stats['total_size_mb']:.2f} MB")
    print(f"å‘½ä¸­ç‡: {stats['hit_rate_percent']}%")
    
    print("\nğŸ” æ¸¬è©¦å¸¶ç·©å­˜çš„RAGåˆ†æå™¨")
    print("-" * 30)
    
    # ç¬¬ä¸€æ¬¡é‹è¡Œï¼ˆæ‡‰è©²æœªå‘½ä¸­ç·©å­˜ï¼‰
    print("ç¬¬ä¸€æ¬¡é‹è¡Œï¼ˆæ‡‰è©²æœªå‘½ä¸­ç·©å­˜ï¼‰:")
    cached_analyzer = CachedRAGAnalyzer(project_path, cache_manager)
    report1 = cached_analyzer.generate_analysis_report()
    print(f"åˆ†æå®Œæˆï¼Œåˆ†æ•¸: {report1.get('overall_assessment', {}).get('overall_score', 0):.1f}/100")
    
    # ç¬¬äºŒæ¬¡é‹è¡Œï¼ˆæ‡‰è©²å‘½ä¸­ç·©å­˜ï¼‰
    print("\nç¬¬äºŒæ¬¡é‹è¡Œï¼ˆæ‡‰è©²å‘½ä¸­ç·©å­˜ï¼‰:")
    report2 = cached_analyzer.generate_analysis_report()
    print(f"å¾ç·©å­˜åŠ è¼‰ï¼Œåˆ†æ•¸: {report2.get('overall_assessment', {}).get('overall_score', 0):.1f}/100")
    
    # é¡¯ç¤ºç·©å­˜çµ±è¨ˆ
    print("\nğŸ“Š æœ€çµ‚ç·©å­˜çµ±è¨ˆ:")
    final_stats = cache_manager.get_stats()
    print(f"å‘½ä¸­æ¬¡æ•¸: {final_stats['hits']}")
    print(f"æœªå‘½ä¸­æ¬¡æ•¸: {final_stats['misses']}")
    print(f"è¨­ç½®æ¬¡æ•¸: {final_stats['sets']}")
    print(f"æœ€çµ‚å‘½ä¸­ç‡: {final_stats['hit_rate_percent']}%")
    
    # åˆ—å‡ºç·©å­˜çš„é …ç›®
    print("\nğŸ“ ç·©å­˜çš„é …ç›®:")
    projects = cache_manager.list_cached_projects()
    for i, project in enumerate(projects[:3], 1):
        print(f"{i}. {project['path']}")
        print(f"   é¡å‹: {', '.join(project['cache_types'])}")
        print(f"   å¤§å°: {project['total_size_mb']:.2f} MB")
        print(f"   æœ€å¾Œè¨ªå•: {project['last_accessed_str']}")
    
    print("\nâœ… ç·©å­˜æ¸¬è©¦å®Œæˆï¼")


if __name__ == "__main__":
    main()